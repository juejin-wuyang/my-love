[
  {
    "title": "画布 1",
    "topic": {
      "title": "零拷贝& mmap",
      "topics": [
        {
          "title": "内存零拷贝技术",
          "topics": [
            {
              "title": "实现方式",
              "topics": [
                {
                  "title": "直接 IO",
                  "topics": [
                    {
                      "title": "直接 I/O：对于这种数据传输方式来说，应用程序可以直接访问硬件存储，操作系统内核只是辅助数据传输。这种方式依旧存在用户空间和内核空间的上下文切换，但是硬件上的数据不会拷贝一份到内核空间，而是直接拷贝至了用户空间，因此直接I/O不存在内核空间缓冲区和用户空间缓冲区之间的数据拷贝。"
                    },
                    {
                      "title": "避免内核空间和用户控件内的数据拷贝",
                      "topics": [
                        {
                          "title": "② 在数据传输过程中，避免数据在用户空间缓冲区和系统内核空间缓冲区之间的CPU拷贝，"
                        },
                        {
                          "title": "共享内核缓冲区到用户地址空间"
                        }
                      ]
                    },
                    {
                      "title": "直接硬件(磁盘)->用户控件缓冲区"
                    }
                  ]
                },
                {
                  "title": "mmap"
                },
                {
                  "title": "sendfile"
                }
              ]
            },
            {
              "title": "场景案例",
              "topics": [
                {
                  "title": "将系统中的文件发送到远端(该流程涉及：
磁盘上文件 ——> 内存(字节数组) ——> 传输给用户/网络)来详细展开传统I/O操作和通过零拷贝来实现的I/O操"
                },
                {
                  "title": "传统IO",
                  "topics": [
                    {
                      "title": "① 发出read系统调用：导致用户空间到内核空间的上下文切换(第一次上下文切换)。通过DMA引擎将文件中的数据从磁盘上读取到内核空间缓冲区(第一次拷贝: hard drive ——> kernel buffer)。"
                    },
                    {
                      "title": " ② 将内核空间缓冲区的数据拷贝到用户空间缓冲区(第二次拷贝: kernel buffer ——> user buffer)，然后read系统调用返回。而系统调用的返回又会导致一次内核空间到用户空间的上下文切换(第二次上下文切换)。"
                    },
                    {
                      "title": " ③ 发出write系统调用：导致用户空间到内核空间的上下文切换(第三次上下文切换)。将用户空间缓冲区中的数据拷贝到内核空间中与socket相关联的缓冲区中(即，第②步中从内核空间缓冲区拷贝而来的数据原封不动的再次拷贝到内核空间的socket缓冲区中。)(第三次拷贝: user buffer ——> socket buffer)。"
                    },
                    {
                      "title": " ④ write系统调用返回，导致内核空间到用户空间的再次上下文切换(第四次上下文切换)。通过DMA引擎将内核缓冲区中的数据传递到协议引擎(第四次拷贝: socket buffer ——> protocol engine)，这次拷贝是一个独立且异步的过程。"
                    },
                    {
                      "title": "总结 read-> 磁盘到内核缓冲区. -> 用户缓冲区,   write 用户缓冲区-> socket缓冲区-> 网卡驱动"
                    },
                    {
                      "title": "传统的I/O操作进行了4次用户空间与内核空间的上下文切换，以及4次数据拷贝。其中4次数据拷贝中包括了2次DMA拷贝和2次CPU拷贝。"
                    },
                    {
                      "title": "tip   内核缓冲区-到磁盘和网卡或者反向 都是 DMA 可以做的, 在用户缓冲区和内核缓冲区都是内存带宽和cpu 消耗"
                    }
                  ]
                },
                {
                  "title": "传统IO问题",
                  "topics": [
                    {
                      "title": " 传统I/O模式为什么将数据从磁盘读取到内核空间缓冲区，然后再将数据从内核空间缓冲区拷贝到用户空间缓冲区了？为什么不直接将数据从磁盘读取到用户空间缓冲区就好？",
                      "topics": [
                        {
                          "title": "传统I/O模式之所以将数据从磁盘读取到内核空间缓冲区而不是直接读取到用户空间缓冲区，是为了减少磁盘I/O操作以此来提高性能。因为OS会根据局部性原理在一次read()系统调用的时候预读取更多的文件数据到内核空间缓冲区中，这样当下一次read()系统调用的时候发现要读取的数据已经存在于内核空间缓冲区中的时候只要直接拷贝数据到用户空间缓冲区中即可，无需再进行一次低效的磁盘I/O操作(注意：磁盘I/O操作的速度比直接访问内存慢了好几个数量级)。"
                        },
                        {
                          "title": "自我总结: read系统调用可以从磁盘中预取一部分数据到内核缓冲区.避免多次随机磁盘读取(局部性原理)"
                        }
                      ]
                    },
                    {
                      "title": "既然系统内核缓冲区能够减少磁盘I/O操作，那么我们经常使用的BufferedInputStream缓冲区又是用来干啥的？",
                      "topics": [
                        {
                          "title": " A: BufferedInputStream的作用是会根据情况自动为我们预取更多的数据到它自己维护的一个内部字节数据缓冲区中，这样做能够减少系统调用的次数以此来提供性能。"
                        }
                      ]
                    },
                    {
                      "title": "总的来说内核空间缓冲区的一大用处是为了减少磁盘I/O操作，因为它会从磁盘中预读更多的数据到缓冲区中。而BufferedInputStream的用处是减少“系统调用”。",
                      "makers": [
                        "priority-1"
                      ]
                    }
                  ]
                }
              ]
            },
            {
              "title": "DMA",
              "topics": [
                {
                  "title": " DMA(Direct Memory Access) ———— 直接内存访问 ：DMA是允许外设组件将I/O数据直接传送到主存储器中并且传输不需要CPU的参与，以此将CPU解放出来去完成其他的事情。"
                },
                {
                  "title": " 而用户空间与内核空间之间的数据传输并没有类似DMA这种可以不需要CPU参与的传输工具，因此用户空间与内核空间之间的数据传输是需要CPU全程参与的。所有也就有了通过零拷贝技术来减少和避免不必要的CPU数据拷贝过程。"
                },
                {
                  "title": "思考: 外设例如磁盘网卡显卡均允许 和内存直接的直接IO,解放 CPU ,但是内存之间的数据拷贝 只能 CPU自己来干"
                }
              ]
            },
            {
              "title": "通过sendfile实现的零拷贝I/O",
              "topics": [
                {
                  "title": "① 发出sendfile系统调用，导致用户空间到内核空间的上下文切换(第一次上下文切换)。通过DMA引擎将磁盘文件中的内容拷贝到内核空间缓冲区中(第一次拷贝: hard drive ——> kernel buffer)。然后再将数据从内核空间缓冲区拷贝到内核中与socket相关的缓冲区中(第二次拷贝: kernel buffer ——> socket buffer)。"
                },
                {
                  "title": " ② sendfile系统调用返回，导致内核空间到用户空间的上下文切换(第二次上下文切换)。通过DMA引擎将内核空间socket缓冲区中的数据传递到协议引擎(第三次拷贝: socket buffer ——> protocol engine)"
                },
                {
                  "title": "通过sendfile实现的零拷贝I/O只使用了2次用户空间与内核空间的上下文切换，以及3次数据的拷贝。其中3次数据拷贝中包括了2次DMA拷贝和1次CPU拷贝。"
                },
                {
                  "title": "但通过是这里还是存在着一次CPU拷贝操作，即，kernel buffer ——> socket buffer。是否有办法将该拷贝操作也取消掉了？",
                  "topics": [
                    {
                      "title": "需要底层操作系统的支持。从Linux 2.4版本开始，操作系统底层提供了scatter/gather这种DMA的方式来从内核空间缓冲区中将数据直接读取到协议引擎中，而无需将内核空间缓冲区中的数据再拷贝一份到内核空间socket相关联的缓冲区中"
                    }
                  ]
                },
                {
                  "title": "可以通过操作系统支持, 通过外设将数据 DMA传输到内核缓冲区,然后再从内核缓冲区-> 到另一个外设的中, 两次 DMA即可"
                },
                {
                  "title": "sendfile()系统调用也会引起用户态到内核态的切换，与内存映射方式不同的是，用户空间此时是无法看到或修改数据内容，也就是说这是一次完全意义上的数据传输过程。",
                  "makers": [
                    "priority-1",
                    "task-done"
                  ],
                  "topics": [
                    {
                      "title": "mmap 是通过将磁盘文件映射到内存, 对内存的修改将写会文件. 可以加速读写文件, 可以实现进程间共享"
                    }
                  ]
                },
                {
                  "title": "缺点",
                  "topics": [
                    {
                      "title": "基于性能的考虑来说，sendfile () 仍然需要有一次从文件到 socket 缓冲区的 CPU 拷贝操作，这就导致页缓存有可能会被传输的数据所污染。导致大量页缓存失效. 应该详细控制 页缓存,防止非活跃数据 占用有限的内存"
                    }
                  ]
                }
              ]
            },
            {
              "title": "带有DMA收集拷贝功能的sendfile实现的I/O",
              "topics": [
                {
                  "title": "Linux 2.4版本开始，操作系统底层提供了带有scatter/gather的DMA来从内核空间缓冲区中将数据读取到协议引擎中。这样一来待传输的数据可以分散在存储的不同位置上，而不需要在连续存储中存放。那么从文件中读出的数据就根本不需要被拷贝到socket缓冲区中去，只是需要将缓冲区描述符添加到socket缓冲区中去，DMA收集操作会根据缓冲区描述符中的信息将内核空间中的数据直接拷贝到协议引擎"
                },
                {
                  "title": "过程",
                  "topics": [
                    {
                      "title": "① 发出sendfile系统调用，导致用户空间到内核空间的上下文切换(第一次上下文切换)。通过DMA引擎将磁盘文件中的内容拷贝到内核空间缓冲区中(第一次拷贝: hard drive ——> kernel buffer)。"
                    },
                    {
                      "title": " ② 没有数据拷贝到socket缓冲区。取而代之的是只有相应的描述符信息会被拷贝到相应的socket缓冲区当中。该描述符包含了两方面的信息：a)kernel buffer的内存地址；b)kernel buffer的偏移量。"
                    },
                    {
                      "title": " ③ sendfile系统调用返回，导致内核空间到用户空间的上下文切换(第二次上下文切换)。DMA gather copy根据socket缓冲区中描述符提供的位置和偏移量信息直接将内核空间缓冲区中的数据拷贝到协议引擎上(第二次拷贝: kernel buffer ——> protocol engine)，这样就避免了最后一次CPU数据拷贝。"
                    },
                    {
                      "title": "总结",
                      "topics": [
                        {
                          "title": "带有DMA收集拷贝功能的sendfile实现的I/O只使用了2次用户空间与内核空间的上下文切换，以及2次数据的拷贝，而且这2次的数据拷贝都是非CPU拷贝。这样一来我们就实现了最理想的零拷贝I/O传输了，不需要任何一次的CPU拷贝，以及最少的上下文切换"
                        },
                        {
                          "title": "通过将内核缓冲区 的描述符拷贝到 socket缓冲区, 这样dma 可以直接访问到对应内核缓冲区的数据,减少了最后一次 CPU io拷贝"
                        }
                      ]
                    }
                  ]
                },
                {
                  "title": "Linux 2.6之后,支持文件-> 文件, 文件到 socket 的数据传输, 即使文件到文件也需要到内核缓冲区做中转"
                },
                {
                  "title": "之前网卡都是从统一的缓冲区读取数据,现在要支持在 非连续的地址 收集式的读取数据. 所以 像mmap 这种虽然映射到了内核态数据,也需要拷贝到socket 缓冲区.因为网卡需要读取连续的地址. 这个机制 不能改变. 有了收集功能另当别论.由 sendfile 支持. 直接用户态感知不到这块数据. mmap还可以感知到这块数据.",
                  "makers": [
                    "task-done",
                    "flag-red",
                    "priority-1"
                  ]
                }
              ]
            },
            {
              "title": "比较",
              "topics": [
                {
                  "title": ""传统I/O” VS “sendfile零拷贝I/O”"
                },
                {
                  "title": "传统I/O通过两条系统指令read、write来完成数据的读取和传输操作，以至于产生了4次用户空间与内核空间的上下文切换的开销；而sendfile只使用了一条指令就完成了数据的读写操作，所以只产生了2次用户空间与内核空间的上下文切换。"
                },
                {
                  "title": "传统I/O产生了2次无用的CPU拷贝，即内核空间缓存中数据与用户空间缓冲区间数据的拷贝；而sendfile最多只产出了一次CPU拷贝，即内核空间内之间的数据拷贝，甚至在底层操作体系支持的情况下，sendfile可以实现零CPU拷贝的I/O。"
                },
                {
                  "title": "因传统I/O用户空间缓冲区中存有数据，因此应用程序能够对此数据进行修改等操作；而sendfile零拷贝消除了所有内核空间缓冲区与用户空间缓冲区之间的数据拷贝过程，因此sendfile零拷贝I/O的实现是完成在内核空间中完成的，这对于应用程序来说就无法对数据进行操作了。"
                },
                {
                  "title": "mmap 和sendfile比较: mmap 经过了多次系统调用, mmap 可以暴露缓冲区给用户态程序."
                }
              ]
            },
            {
              "title": "通过mmap实现的零拷贝I/O",
              "topics": [
                {
                  "title": "过程",
                  "topics": [
                    {
                      "title": "① 发出mmap系统调用，导致用户空间到内核空间的上下文切换(第一次上下文切换)。通过DMA引擎将磁盘文件中的内容拷贝到内核空间缓冲区中(第一次拷贝: hard drive ——> kernel buffer)。"
                    },
                    {
                      "title": " ② mmap系统调用返回，导致内核空间到用户空间的上下文切换(第二次上下文切换)。接着用户空间和内核空间共享这个缓冲区，而不需要将数据从内核空间拷贝到用户空间。因为用户空间和内核空间共享了这个缓冲区数据，所以用户空间就可以像在操作自己缓冲区中数据一般操作这个由内核空间共享的缓冲区数据。"
                    },
                    {
                      "title": " ③ 发出write系统调用，导致用户空间到内核空间的上下文切换(第三次上下文切换)。将数据从内核空间缓冲区拷贝到内核空间socket相关联的缓冲区(第二次拷贝: kernel buffer ——> socket buffer)。"
                    },
                    {
                      "title": " ④ write系统调用返回，导致内核空间到用户空间的上下文切换(第四次上下文切换)。通过DMA引擎将内核空间socket缓冲区中的数据传递到协议引擎(第三次拷贝: socket buffer ——> protocol engine)"
                    },
                    {
                      "title": "通过mmap实现的零拷贝I/O进行了4次用户空间与内核空间的上下文切换，以及3次数据拷贝。其中3次数据拷贝中包括了2次DMA拷贝和1次CPU拷贝"
                    }
                  ]
                },
                {
                  "title": "使用mmap 利用内核缓冲区 时,也是需要将内核缓冲区的数据 拷贝到socket 缓冲区. (不仅仅是为了安全.而是一个机制的问题, 优化性能可能会破坏通用性)",
                  "makers": [
                    "task-done",
                    "flag-red"
                  ]
                }
              ]
            },
            {
              "title": "splice 实现 ",
              "topics": [
                {
                  "title": "可以用于用户应用程序地址空间和操作系统地址空间之间的数据传输。splice() 适用于可以确定数据传输路径的用户应用程序，它不需要利用用户地址空间的缓冲区进行显式的数据传输操作"
                },
                {
                  "title": "当数据只是从一个地方传送到另一个地方，过程中所传输的数据不需要经过用户应用程序的处理的时候，spice() 就成为了一种比较好的选择。splice() 可以在操作系统地址空间中整块地移动数据，从而减少大多数数据拷贝操作。而且，splice() 进行数据传输可以通过异步的方式来进行，用户应用程序可以先从系统调用返回，而操作系统内核进程会控制数据传输过程继续进行下去(使用信号通知传输结束)。splice() 可以被看成是类似于基于流的管道的实现，管道可以使得两个文件描述符相互连接，splice 的调用者则可以控制两个设备（或者协议栈）在操作系统内核中的相互连接。"
                },
                {
                  "title": "splice() 系统调用和 sendfile() 非常类似，用户应用程序必须拥有两个已经打开的文件描述符，一个用于表示输入设备，一个用于表示输出设备。与 sendfile() 不同的是，splice() 允许任意两个文件之间互相连接，而并不只是文件到 socket 进行数据传输。对于从一个文件描述符发送数据到 socket 这种特例来说，一直都是使用 sendfile() 这个系统调用，而 splice 一直以来就只是一种机制，它并不仅限于 sendfile() 的功能。也就是说，sendfile() 只是 splice() 的一个子集，在 Linux 2.6.23 中，sendfile() 这种机制的实现已经没有了，但是这个 API 以及相应的功能还存在，只不过 API 以及相应的功能是利用了 splice() 这种机制来实现的。"
                },
                {
                  "title": "在数据传输的过程中，splice() 机制交替地发送相关的文件描述符的读写操作，并且可以将读缓冲区重新用于写操作。它也利用了一种简单的流控制，通过预先定义的水印（ watermark ）来阻塞写请求。有实验表明，利用这种方法将数据从一个磁盘传输到另一个磁盘会增加 30% 到 70% 的吞吐量，数据传输的过程中， CPU 的负载也会减少一半。"
                },
                {
                  "title": "	
long splice(int fdin, int fdout, size_t len, unsigned int flags);",
                  "topics": [
                    {
                      "title": "flag 参数",
                      "topics": [
                        {
                          "title": "SPLICE_F_NONBLOCK：splice 操作不会被阻塞。然而，如果文件描述符没有被设置为不可被阻塞方式的 I/O ，那么调用 splice 有可能仍然被阻塞。"
                        },
                        {
                          "title": "SPLICE_F_MORE：告知操作系统内核下一个 splice 系统调用将会有更多的数据传来。"
                        },
                        {
                          "title": "SPLICE_F_MOVE：如果输出是文件，这个值则会使得操作系统内核尝试从输入管道缓冲区直接将数据读入到输出地址空间，这个数据传输过程没有任何数据拷贝操作发生。"
                        }
                      ]
                    }
                  ]
                },
                {
                  "title": "调用 splice() 系统调用会导致操作系统内核从数据源 fdin 移动最多 len 个字节的数据到 fdout 中去，这个数据的移动过程只是经过操作系统内核空间，需要最少的拷贝次数。使用 splice() 系统调用需要这两个文件描述符中的一个必须是用来表示一个管道设备的。"
                }
              ]
            }
          ]
        },
        {
          "title": "buffer io/ direct io",
          "topics": [
            {
              "title": "buffer IO",
              "topics": [
                {
                  "title": "大多数文件系统的默认 I/O 操作都是缓存 I/O"
                },
                {
                  "title": "步骤",
                  "topics": [
                    {
                      "title": "磁盘DMA 读取数据到内核缓冲区 也就是page cache"
                    },
                    {
                      "title": "操作系统内核的缓冲区拷贝到应用程序的地址空间"
                    },
                    {
                      "title": "用户态库使用这些数据(对实际应用代码可能还会提供缓存)"
                    }
                  ]
                },
                {
                  "title": "好处",
                  "topics": [
                    {
                      "title": "缓存 I/O 使用了操作系统内核缓冲区，在一定程度上分离了应用程序空间和实际的物理设备。"
                    },
                    {
                      "title": "缓存 I/O 可以减少读盘的次数，从而提高性能。"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "title": "mmap",
          "topics": [
            {
              "title": "概述",
              "topics": [
                {
                  "title": "将文件的一部分映射到调用进程的虚拟内存中.",
                  "topics": [
                    {
                      "title": "实际是将进程虚拟地址映射到page cache物理映射"
                    },
                    {
                      "title": "也可以使用匿名映射,不指定文件"
                    }
                  ]
                },
                {
                  "title": "进程共享内存的一种方式"
                }
              ]
            },
            {
              "title": "私有映射",
              "topics": [
                {
                  "title": "映射的内容对其他进程不可见"
                },
                {
                  "title": "文件映射的话, 修改也不会变更到文件, 即使 多个进程使用私有映射, 初始时共享了此块内存, 但是后续变更也不会对其他进程可见, linux使用了写时复制技术, 对于变更的修改,会重新分配内存"
                },
                {
                  "title": "私有匿名映射:常用来分配一块零初始化内存"
                },
                {
                  "title": "私有文件映射,常用于可执行文件或共享库的初始化数据端. 既可以将文件内容映射到内存,修改变更又不会同步到文件"
                }
              ]
            },
            {
              "title": "共享映射",
              "topics": [
                {
                  "title": "对其他进程可见, 也会修改到文件."
                },
                {
                  "title": "匿名映射: 子进程在fork时,会继承这个映射,在父子进程之间实现内存共享"
                },
                {
                  "title": "目前只是父子进程会继承,可以实现共享匿名映射. 如果基于文件,可以实现多个进程共享"
                }
              ]
            },
            {
              "title": "API介绍",
              "topics": [
                {
                  "title": "创建映射",
                  "topics": [
                    {
                      "title": "可以指定映射的首地址(虚拟空间地址, 常常设置null,让内核来分配),该其实地址会进行分页地址对齐"
                    },
                    {
                      "title": "指定offset, length可以指定文件的哪一块内存被映射"
                    }
                  ]
                },
                {
                  "title": "解除映射",
                  "topics": [
                    {
                      "title": "munmap",
                      "topics": [
                        {
                          "title": "只需要传入内存映射的地址"
                        }
                      ]
                    },
                    {
                      "title": "解除映射后mlock 对内存的锁定(要求常驻内存)将会失效."
                    },
                    {
                      "title": "exec之后,进程原有的内存映射全部失效"
                    },
                    {
                      "title": "为了确保解除映射前,文件修改全部写入底层文件, 要调用msync刷新page cache"
                    }
                  ]
                },
                {
                  "title": "参数",
                  "topics": [
                    {
                      "title": "默认按照mlock 锁定内存页"
                    },
                    {
                      "title": "超前读取. 后续对映射内容的访问,不会出现分页缺陷.(linux 读取时才会从文件中加载进缓存)"
                    }
                  ]
                },
                {
                  "title": "mremap可重新映射"
                }
              ]
            },
            {
              "title": "内存映射IO",
              "topics": [
                {
                  "title": "常用于 文件IO优化, 适合于大量随机访问的场景. 可以不使用read, write的方式访问内存.",
                  "topics": [
                    {
                      "title": "减少一次内核态到用户态数据的访问."
                    },
                    {
                      "title": "小数据量IO 由于映射, 分页故障, 解除映射,等.消耗会相比性能提升更费性能.得不偿失了."
                    }
                  ]
                }
              ]
            },
            {
              "title": "边界场景",
              "topics": [
                {
                  "title": "当映射数组,小于文件长度,但是却访问了数组的越界部分,则会出现内存错误, 实际是内核发送SIGSEGV,默认是终止进程,core dump"
                },
                {
                  "title": "当映射数组大于文件, 访问无法映射到文件的部分时, 即使修改了,相关修改也不会被映射到文件.",
                  "topics": [
                    {
                      "title": "没有被映射到文件,也不会被共享"
                    },
                    {
                      "title": "可以使用ftruncate增大文件大小,就可以使用映射了"
                    }
                  ]
                }
              ]
            },
            {
              "title": "msync",
              "topics": [
                {
                  "title": "刷新page cache到磁盘"
                },
                {
                  "title": "可同步刷盘, 也可以异步刷盘(由pflush负责)"
                }
              ]
            },
            {
              "title": "oom杀手",
              "topics": [
                {
                  "title": "内核会尝试杀死缓解内存消耗状况的最佳进程"
                },
                {
                  "title": "一般不会杀死一下进程",
                  "topics": [
                    {
                      "title": "特权进程"
                    },
                    {
                      "title": "正在访问裸设备的进程"
                    },
                    {
                      "title": "已经运行了很长时间,消耗大量cpu的进程.因为杀死他们,会丢失很多工作"
                    }
                  ]
                },
                {
                  "title": "发送SIGKILL信号",
                  "topics": [
                    {
                      "title": "子主题 1"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "title": "虚拟内存操作",
          "topics": [
            {
              "title": "mprotect",
              "topics": [
                {
                  "title": "修改内存保护位"
                }
              ]
            },
            {
              "title": "mlock",
              "topics": [
                {
                  "title": "可以保证内存页不被换出",
                  "topics": [
                    {
                      "title": "对于低延迟的程序,为了避免出现陡然的io延迟"
                    },
                    {
                      "title": "保证敏感信息不被替换到磁盘"
                    }
                  ]
                },
                {
                  "title": "特权级进程锁住的内存数量没有限制"
                },
                {
                  "title": "接受传入的内存地址,会进行分页对齐"
                },
                {
                  "title": "内存锁不会再单个进程上叠加. "
                },
                {
                  "title": "不会被fork继承"
                }
              ]
            },
            {
              "title": "mincore",
              "topics": [
                {
                  "title": "确定内存分页是否在内存中"
                }
              ]
            },
            {
              "title": "madvise",
              "topics": [
                {
                  "title": "建议内核后续的内存使用方式, 决定内核是否需要 预先读, 是否会常驻内存, 是不是只会访问一次"
                }
              ]
            },
            {
              "title": "shmctl",
              "topics": [
                {
                  "title": "只会当内存页发生缺页故障,才会加载进内存,并且被锁住."
                },
                {
                  "title": "即使锁住的进程退出也不会导致该内存页解锁."
                },
                {
                  "title": "这个锁和mlock不同, mlock是进程级别的锁. shmctl是系统级的锁."
                }
              ]
            },
            {
              "title": "可支持将之前所有映射的内存加锁, 也支持后续待分配的映射加锁. 也支持所有都加锁"
            }
          ]
        }
      ]
    },
    "structure": "org.xmind.ui.map.unbalanced"
  }
]